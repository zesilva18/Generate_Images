{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYP9yZkmHADm"
      },
      "source": [
        "<div style=\"width: 100%; overflow: hidden;\">\n",
        "    <a href=\"http://www.uc.pt/fctuc/dei/\">\n",
        "    <div style=\"display: block;margin-left: auto;margin-right: auto; width: 50%;\"><img src=\"https://eden.dei.uc.pt/~naml/images_ecos/dei25.png\"  /></div>\n",
        "    </a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-04T09:18:55.583235Z",
          "start_time": "2021-01-04T09:18:52.571712Z"
        },
        "id": "RZgcyKMLHADp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the directory where your images are\n",
        "image_directory = 'data-students\\\\TRAIN'\n",
        "\n",
        "# # Define the directory where your images are in Google Drive\n",
        "# image_directory = '/content/drive/My Drive/your_folder_name/TRAIN'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the directory where your images are\n",
        "image_directory = 'data-students\\\\TRAIN'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "train_dataset = ImageFolder(root=image_directory, transform=transform)\n",
        "\n",
        "\n",
        "print(train_dataset.class_to_idx)  # Mostra todas as classes e seus índices\n",
        "\n",
        "# Suponha que você quer apenas a classe cujo nome é 'class_name'\n",
        "target_class = '6'\n",
        "target_class_index = train_dataset.class_to_idx[target_class]\n",
        "\n",
        "# Filtrar índices\n",
        "target_indices = [i for i, (img, label) in enumerate(train_dataset) if label == target_class_index]\n",
        "\n",
        "# Criar subset apenas com a classe desejada\n",
        "target_dataset = Subset(train_dataset, target_indices)\n",
        "\n",
        "# DataLoader para o subset\n",
        "train_loader = DataLoader(target_dataset, batch_size=128, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## show some images from train_loader\n",
        "\n",
        "for images, labels in train_loader:\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Training Images\")\n",
        "    plt.imshow(np.transpose(torchvision.utils.make_grid(images, nrow=16).cpu(), (1, 2, 0)))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_images(images):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    for i in range(images.shape[0]):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(np.transpose(images[i].detach().cpu().numpy(), (1, 2, 0)), interpolation='nearest')\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOMW0S1gHADr"
      },
      "source": [
        "<h2><font color='#172241'>1. Introduction</font></h2>\n",
        "\n",
        "In this class we are going to discuss and implement Generative Adversarial Networks (GANs). A GAN combines two deep neural networks: a discriminator D and a generator G. The generator G receives a noise as input and outputs a fake sample, trying to replicate the data distribution used as input for D. The discriminator D receives the real data and fake samples as input, and tries to distinguish between them. These components are trained simultaneously as adversaries, hopefully creating strong generative and discriminative components. Then image bellow presents an overview of the entire process.\n",
        "\n",
        "<img src=\"GANDiagram.png\">\n",
        "\n",
        "\n",
        "Over the years several GANs models and architectures have been proposed in the literature [\\[1\\]](https://arxiv.org/abs/1701.07875)[\\[2\\]](https://arxiv.org/abs/1912.04958). These models are quite advanced and produce impressive results but they require large ammounts of computational resources.\n",
        "\n",
        "In our case we are going to implement a Deep Convolutional GAN (DCGAN). This specific model assumes that the Generator and de Discriminator are Deep Convolutional Neural Networks. In particular, the Generator makes use of a [Conv2DTranspose](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) layer for upsampling the images.\n",
        "\n",
        "We will create a GAN to replicate images from the CIFAR-10 dataset [\\[1\\]](https://www.cs.toronto.edu/~kriz/cifar.html)[\\[2\\]](https://www.cs.toronto.edu/~kriz/conv-cifar10-aug2010.pdf). \n",
        "\n",
        "The CIFAR-10 dataset is highly used by state-of-the art methods, allowing a comparison between different CNN architectures. The datasets is composed of 60000 32x32 RGB images, where each image is assigned to one of 10 classes: \n",
        "- 'airplane', \n",
        "- 'automobile'\n",
        "- 'bird' \n",
        "- 'cat' \n",
        "- 'deer'\n",
        "- 'dog'\n",
        "- 'frog'\n",
        "- 'horse'\n",
        "- 'ship'\n",
        "- 'truck'\n",
        "\n",
        "The 10 are non-overlapping, meaning that there are exactly 6000 images per class. The dataset is split into train and test sets, with 50000 images used for training and the remaining 10000 images used for testing. The test set contains exactly 1000 randomly-selected images from each class. \n",
        "\n",
        "\n",
        "To simplify we will only use images from the class `cat`.\n",
        "\n",
        "To train a GAN: \n",
        "- The generator maps a random vector x,  the latent space, of shaoe (latent_dim,) to images of shape (32,32,3)\n",
        "- The discriminator network receives images of shape (32,32,3) from the the discriminator and the real dataset, and produces a binary score estimating the probability of each image being real or false.\n",
        "- The GAN network joins the generator and the discriminator, i.e., GAN = D(G(x)). The GAN uses the generator to map the latent space to the descriminator, which will asssess the realism of the latent vectors as decoded by Generator.\n",
        "- The Discriminator is trainined on fake and real images\n",
        "- The generator is trainined using the gradients of the generator's weights with regard to the loss of the GAN model. This means that, at every step, we move the weights of the generator in a direction that will make the discriminator more likely to classify as \"real\" the images decoded by the generator. I.e. we train the generator to fool the discriminator.\n",
        "\n",
        "\n",
        "### Some Problem and Some Tricks...\n",
        "Building and training GANs is extremely difficult. Two common problems regarding training of GANs are the vanishing gradient and mode collapse. The vanishing gradient occurs when the discriminator D became perfect and do not commit mistakes anymore. Hence, the loss function is zeroed, the gradient does not flow through the neural network of the generator, and the GAN progress stagnates. In mode collapse, the generator captures only a small portion of the dataset distribution provided as input to the discriminator. This is not desirable once we want to reproduce the whole distribution of the data.\n",
        "\n",
        "Over the years some tips and tricks have been proposed to help build GANs. Keep in mind that most of these tricks are just expert knowledge from people that have spent countless hours working on these models. \n",
        "\n",
        "Here are a few tricks [\\[3\\]](https://www.manning.com/books/deep-learning-with-python):\n",
        "- `tanh` should be used as the last activation in the Generator, instead of `sigmoid\n",
        "- The latent space is created using a normal distribution (Gaussian distribution), not a uniform distribution\n",
        "- Stochasticity is good. Since GAN training results in a dynamic equilibrium, GANs are likely to get \"stuck\" in all sorts of ways. Introducing randomness during training helps prevent this. We introduce randomness in two ways: 1) we use dropout in the discriminator, 2) we add some random noise to the labels for the discriminator.\n",
        "- Sparse gradients, i.e. when the network does not receive enough signals to adjust its weights, can hinder GAN training. There are two things that can induce gradient sparsity: 1) max pooling operations, 2) ReLU activations. Instead of max pooling, we recommend using strided convolutions for downsampling, and we recommend using a LeakyReLU layer instead of a ReLU activation. It is similar to ReLU but it relaxes sparsity constraints by allowing small negative activation values.\n",
        "- In generated images, it is common to see \"checkerboard artifacts\" caused by unequal coverage of the pixel space in the generator. To fix this, we use a kernel size that is divisible by the stride size, whenever we use a strided Conv2DTranpose or Conv2D in both the generator and discriminator.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGZJfEdVHADu"
      },
      "source": [
        "### The Generator\n",
        "\n",
        "Lets start by building the generator model. It turns a vector (from the latent space -- during training it will sampled at random) into a candidate image. One of the many issues that commonly arise with GANs is that the generator gets stuck with generated images that look like noise. A possible solution is to use dropout on both the discriminator and generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUirk6uOHADu"
      },
      "outputs": [],
      "source": [
        "#torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True,\n",
        "# padding_mode='zeros', device=None, dtype=None)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            \n",
        "            nn.ConvTranspose2d(100, 1024, 4, 1, 0, bias=False),  # Saída: (512, 4, 4)\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(True),\n",
        "            \n",
        "            nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),  \n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),  # Saída: (256, 8, 8)\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),  # Saída: (128, 16, 16)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 3, 4, 2, 1, bias=False),     # Saída: (3, 64, 64)\n",
        "            nn.Tanh()  # Normaliza a saída para [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "    \n",
        "    # def forward(self, input):\n",
        "    #     for layer in self.main:\n",
        "    #         input = layer(input)\n",
        "    #         print(input.shape)  # print the output shape after each layer\n",
        "    #     return input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # PARA VER AS SHAPES DO GENERATOR E AJUSTAR O DISCRIMINATOR\n",
        "# # # Create an instance of the Generator class\n",
        "# gen = Generator()\n",
        "\n",
        "# # Create a random input tensor\n",
        "# input_tensor = torch.randn(1, 100, 1, 1)\n",
        "\n",
        "# # Call the forward method\n",
        "# output = gen.forward(input_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcA9AMHAHADw"
      },
      "source": [
        "### The Discriminator\n",
        "\n",
        "The Discriminator model, that takes as input a candidate image (real or synthetic) and classifies it into one of two classes, either \"generated image\" or \"real image that comes from the training set\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5LG2Zq2HADw"
      },
      "outputs": [],
      "source": [
        "#torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True,\n",
        "# padding_mode='zeros', device=None, dtype=None)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, 4, 2, 1, bias=False),  # Saída: (64, 32, 32)\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),  # Saída: (128, 16, 16)\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),  # Saída: (256, 8, 8)\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Conv2d(512, 1024, 4, 2, 1, bias=False),  # Saída: (512, 4, 4)\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Conv2d(1024, 1, 4, 1, 0, bias=False),    # Saída: (1, 1, 1)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input).view(-1, 1).squeeze(1)\n",
        "\n",
        "    # def forward(self, input):\n",
        "    #     print(f'Input shape: {input.shape}')  # print the shape of the input\n",
        "        \n",
        "    #     for layer in self.model:\n",
        "    #         input = layer(input)\n",
        "    #         print(f'Output shape: {input.shape}')  # print the output shape after each layer\n",
        "            \n",
        "    #     final_output = input.view(-1, 1).squeeze(1)\n",
        "    #     print(f'Final output shape: {final_output.shape}')  # print the shape of the final output\n",
        "    #     return final_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Create an instance of the Discriminator class\n",
        "# disc = Discriminator()\n",
        "\n",
        "# # Create a random input tensor that matches the output shape of the generator, last value should be the last from generator (x, y, Z, Z)\n",
        "# input_tensor = torch.randn(128, 3, 64, 64)\n",
        "\n",
        "# # Call the forward method\n",
        "# output = disc.forward(input_tensor)\n",
        "\n",
        "# # result should be batch_size, 1, 1, 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiB-XDCFHADy"
      },
      "source": [
        "### The Adversarial Network\n",
        "Finally, we need to chaning the Generator and the Discriminator, i.e., create the GAN. \n",
        "It will turn latent space points into a classification decision, `fake` or `real`, and it is meant to be trained with labels that are always \"these are real images\". So training the GAN will update the weights of generator in a way that makes discriminator more likely to predict \"real\" when looking at fake images. Very importantly, we set the discriminator to be frozen during training (non-trainable): its weights will not be updated when training gan. If the discriminator weights could be updated during this process, then we would be training the discriminator to always predict \"real\", which is not what we want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsVN8SQTHADy",
        "outputId": "716fb59b-0f9f-4491-c5ef-843226c9448b"
      },
      "outputs": [],
      "source": [
        "netG = Generator().to(device)\n",
        "netD = Discriminator().to(device)\n",
        "\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=0.0001, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKxK-dHpHADz"
      },
      "source": [
        "### How to Train the DCGAN\n",
        "- Load the Dataset\n",
        "- Get all the images for the cat class\n",
        "- Normalize the images do all pixels are between 0 and 1\n",
        "- Define:\n",
        "    - Maximum number of iterations\n",
        "    - Batch size\n",
        "\n",
        "- for each batch in epoch:\n",
        "    for k steps:\n",
        "        \n",
        "        1 - Randomly generated a sample from the latent space using a normal distribution with size=(batch_size, latent_dim).\n",
        "    \n",
        "        2 - Generate images with `Generator` using the latent space defined in the previous step (1.).\n",
        "        \n",
        "        3 - Combine the generated images from point 2 with real images from the training dataset.\n",
        "        \n",
        "        4 - Train the `Discriminator` using the combine batch of images, with corresponding targets, either \"real\" or \"fake\".\n",
        "\n",
        "    5 - Generate sample from the latent space latent space using a normal distribution with size=(batch_size, latent_dim). \n",
        "\n",
        "    6 -Train the `Generator` using the samples from 5. the generated images labelled as  \"real\". This will update the weights of the `Generator` only to move them towards getting the `Discriminator` to predict \"these are real images\" for generated images, i.e. this trains the generator to fool the discriminator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "0v6_nX9FHADz",
        "outputId": "c6475eff-b71e-4445-82aa-8ca75420570b"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "\n",
        "real_label = 0.9\n",
        "fake_label = 0.1\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (data, _) in enumerate(train_loader):\n",
        "\n",
        "        netD.zero_grad()\n",
        "        real_cpu = data.to(device)\n",
        "        batch_size = real_cpu.size(0)\n",
        "        label = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
        "\n",
        "        output = netD(real_cpu)\n",
        "        errD_real = criterion(output, label)\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        noise = torch.randn(batch_size, 100, 1, 1, device=device)\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "\n",
        "        output = netD(fake.detach())\n",
        "\n",
        "        errD_fake = criterion(output, label)\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "\n",
        "        netG.zero_grad()\n",
        "        noise = torch.randn(batch_size, 100, 1, 1, device=device)\n",
        "        fake = netG(noise)\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        output = netD(fake)\n",
        "        errG = criterion(output, label)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        optimizerG.step()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(train_loader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "    # Save outputs\n",
        "\n",
        "\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "real_cpu.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(netG.state_dict(), './netG.pth')\n",
        "torch.save(netD.state_dict(), './netD.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1aJ_yd0HAD0"
      },
      "source": [
        "### Generate images using  the trained generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7Utt2fcHAD1"
      },
      "outputs": [],
      "source": [
        "def generate_images(generator, num_images):\n",
        "    with torch.no_grad():  # Temporarily set all the requires_grad flag to false\n",
        "        noise = torch.randn(num_images, 100, 1, 1, device=device)  # 100 is the size of the noise vector\n",
        "        generated_images = generator(noise)\n",
        "        generated_images = (generated_images + 1) / 2  # Rescale images from [-1, 1] to [0, 1]\n",
        "        return generated_images\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images = generate_images(netG, 16)  # Generate 16 images\n",
        "show_images(images)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "6.3-GANs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
