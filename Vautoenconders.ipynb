{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_msssim import ssim\n",
    "from torchvision.utils import save_image\n",
    "from pytorch_fid import fid_score\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import inception_v3\n",
    "from scipy.stats import entropy\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import inception_v3\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from scipy.linalg import sqrtm\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "rebuild_path = 'rebuild'\n",
    "\n",
    "# Verificar se a pasta existe\n",
    "if os.path.exists(rebuild_path) and os.path.isdir(rebuild_path):\n",
    "    # Eliminar a pasta e todo o seu conteúdo\n",
    "    shutil.rmtree(rebuild_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = 'TRAIN_augmented'\n",
    "\n",
    "IMG_WIDTH = 75\n",
    "IMG_HEIGHT = 75\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_WIDTH, IMG_HEIGHT)),   # Redimensiona todas as imagens para 75x75 pixels\n",
    "    transforms.ToTensor(),         # Converte as imagens para tensores do PyTorch\n",
    "\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=train_dataset_path, transform=transform)\n",
    "\n",
    "data_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_images(images, title):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    for img, ax in zip(images[:5], axes):\n",
    "        img = img.permute(1, 2, 0)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceber o dataset\n",
    "\n",
    "traffic_signals_dataset = datasets.ImageFolder(root=train_dataset_path)\n",
    "print(\"Classes encontradas:\", traffic_signals_dataset.classes)\n",
    "\n",
    "class_count_true = {}\n",
    "\n",
    "for _, label in traffic_signals_dataset:\n",
    "    \n",
    "    if label not in class_count_true:\n",
    "        class_count_true[label] = 0\n",
    "        \n",
    "    class_count_true[label] += 1\n",
    "    \n",
    "print(class_count_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, color_channels=3, num_labels=10, latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.latent_dim = latent_dim\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(color_channels + num_labels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128 * 19 * 19, 128)\n",
    "        self.fc_log_var = nn.Linear(128 * 19 * 19, 128)\n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(128 + num_labels, 128 * 19 * 19)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, color_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var) # calculo do desvio padrão\n",
    "        eps = torch.randn_like(std) # gera uma variável aleatória epsilon\n",
    "        return mu + eps * std # vetor latente \n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # Encode, processo de codificação\n",
    "        labels_onehot = F.one_hot(labels, num_classes=self.num_labels).float()\n",
    "        labels_onehot = labels_onehot.unsqueeze(-1).unsqueeze(-1)\n",
    "        labels_onehot = labels_onehot.expand(-1, -1, x.size(2), x.size(3))\n",
    "        x = torch.cat((x, labels_onehot), dim=1)\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_log_var(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        # Decode, processo de descodificação\n",
    "        z = torch.cat((z, F.one_hot(labels, num_classes=self.num_labels).float()), dim=1)\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, 128, 19, 19)\n",
    "        x = self.decoder(x)\n",
    "        return x, mu, log_var\n",
    "\n",
    "    def decode(self, z, labels):\n",
    "        \n",
    "        # Função para decodificar z com labels fornecidas\n",
    "        labels_onehot = F.one_hot(labels, num_classes=self.num_labels).float()\n",
    "        z = torch.cat((z, labels_onehot), dim=1)\n",
    "        z = self.decoder_input(z)\n",
    "        z = z.view(-1, 128, 19, 19)\n",
    "        return self.decoder(z)\n",
    "    \n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    \n",
    "    # Função de perda do VAE\n",
    "    BCE = torch.nn.functional.binary_cross_entropy(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) / mu.size(0)\n",
    "    return BCE + KLD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CVAE(color_channels=3, num_labels=len(full_dataset.classes)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "bce_list = []\n",
    "kld_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Inicializar as variáveis de perda\n",
    "    total_bce = 0\n",
    "    total_kld = 0\n",
    "    \n",
    "    \n",
    "    for idx, (data, labels) in enumerate(data_loader):\n",
    "        \n",
    "        # Mover os dados e labels para o dispositivo\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Passa os dados pelo modelo e obtém a reconstrução, mu e log_var\n",
    "        recon_batch, mu, log_var = model(data, labels) \n",
    "        BCE = torch.nn.functional.binary_cross_entropy(recon_batch, data, reduction='mean')\n",
    "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) / data.shape[0]  # per sample\n",
    "        loss = BCE + KLD\n",
    "        \n",
    "        # Retropropagação e parâmetros de atualização\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculo dos valores BCE KLD\n",
    "        total_bce += BCE.item()\n",
    "        total_kld += KLD.item()\n",
    "    \n",
    "    bce_list.append(total_bce / len(data_loader))\n",
    "    kld_list.append(total_kld / len(data_loader))\n",
    "    \n",
    "    print(f'Epoch {epoch}, BCE: {total_bce / len(data_loader)}, KLD: {total_kld / len(data_loader)}')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_epochs), bce_list, label='BCE Loss')\n",
    "plt.plot(range(num_epochs), kld_list, label='KLD Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# save the model\n",
    "model.load_state_dict(torch.load('cvae_model.pth'))\n",
    "model.to(device)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular Inception Score\n",
    "def inception_score(imgs, device, batch_size=32, resize=True, splits=10):\n",
    "    N = len(imgs)\n",
    "    \n",
    "    # Inception v3 model\n",
    "    inception_model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "    inception_model.eval()\n",
    "    \n",
    "    def get_pred(x):\n",
    "        if resize:\n",
    "            x = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        x = inception_model(x)\n",
    "        return F.softmax(x, dim=1).data.cpu().numpy()\n",
    "\n",
    "    preds = np.zeros((N, 1000))\n",
    "\n",
    "    dataloader = DataLoader(TensorDataset(imgs), batch_size=batch_size)\n",
    "\n",
    "    for i, batch in enumerate(dataloader, 0):\n",
    "        batch = batch[0].to(device)\n",
    "        batch_size_i = batch.size()[0]\n",
    "\n",
    "        preds[i*batch_size:i*batch_size + batch_size_i] = get_pred(batch)\n",
    "\n",
    "    split_scores = []\n",
    "\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores = []\n",
    "        for i in range(part.shape[0]):\n",
    "            pyx = part[i, :]\n",
    "            scores.append(entropy(pyx, py))\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "    return np.mean(split_scores), np.std(split_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geração de imagens para calcular Inception Score\n",
    "num_image = 100\n",
    "model.eval()\n",
    "generated_images = []\n",
    "with torch.no_grad():\n",
    "    for i in range(num_image):\n",
    "        z = torch.randn(1, 128).to(device)\n",
    "        label = torch.randint(0, 10, (1,)).to(device)\n",
    "        sample = model.decode(z, label).cpu()\n",
    "        generated_images.append(sample)\n",
    "\n",
    "generated_images = torch.cat(generated_images, dim=0)\n",
    "generated_images = generated_images.view(num_image, 3, 75, 75)\n",
    "\n",
    "# Calcular Inception Score\n",
    "mean, std = inception_score(generated_images, device)\n",
    "print(f'Inception Score: {mean} +/- {std}')\n",
    "\n",
    "# Preparar o objeto FID\n",
    "fid = FrechetInceptionDistance(feature=2048).to(device)\n",
    "\n",
    "# Carregar imagens reais do DataLoader\n",
    "real_images = []\n",
    "for i, (data, _) in enumerate(data_loader):\n",
    "    real_images.append(data)\n",
    "    if len(real_images) * data.shape[0] >= num_image:\n",
    "        break\n",
    "\n",
    "real_images = torch.cat(real_images, dim=0)[:num_image]\n",
    "\n",
    "# Converta as imagens para uint8\n",
    "real_images = (real_images * 255).byte()\n",
    "generated_images = (generated_images * 255).byte()\n",
    "\n",
    "# Mover para o dispositivo (GPU/CPU)\n",
    "real_images = real_images.to(device)\n",
    "generated_images = generated_images.to(device)\n",
    "\n",
    "# Calcular FID\n",
    "fid.update(real_images, real=True)\n",
    "fid.update(generated_images, real=False)\n",
    "\n",
    "fid_score = fid.compute()\n",
    "print(f'FID Score: {fid_score.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_plot_images(model, device, num_labels, num_images=5):\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(num_labels, num_images, figsize=(num_images * 1, num_labels * 1))\n",
    "    \n",
    "    for i in range(num_labels):\n",
    "        for j in range(num_images):\n",
    "            z = torch.randn(1, 128).to(device)\n",
    "            label = torch.tensor([i]).to(device)  # Label desejada\n",
    "            with torch.no_grad():\n",
    "                image = model.decode(z, label)\n",
    "            image = image.cpu().view(3, 75, 75)\n",
    "            ax = axes[i, j]\n",
    "            ax.imshow(image.permute(1, 2, 0))  # Ajustar a permutação para visualização\n",
    "            ax.axis('off')\n",
    "        axes[i, 0].set_ylabel(f'Label {i}')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()\n",
    "\n",
    "generate_and_plot_images(model, device, num_labels=10)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
