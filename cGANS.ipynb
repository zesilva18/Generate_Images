{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import os\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Arguments\n",
    "BATCH_SIZE = 64\n",
    "Z_DIM = 100\n",
    "LABEL_EMBED_SIZE = 5\n",
    "NUM_CLASSES = 10\n",
    "IMGS_TO_DISPLAY_PER_CLASS = 20\n",
    "LOAD_MODEL = False\n",
    "\n",
    "DB = 'SVHN'\n",
    "\n",
    "CHANNELS = 3\n",
    "EPOCHS = 200\n",
    "\n",
    "# Define the directory where your images are\n",
    "image_directory = 'data-students\\\\TRAIN'\n",
    "\n",
    "# # Define the directory where your images are in Google Drive\n",
    "# image_directory = '/content/drive/My Drive/your_folder_name/TRAIN'\n",
    "\n",
    "# Directories for storing data, model and output samples\n",
    "model_path = os.path.join('./model', DB)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "samples_path = os.path.join('./samples', DB)\n",
    "os.makedirs(samples_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "transform = transforms.Compose([transforms.Resize([64, 64]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=image_directory, transform=transform)\n",
    "\n",
    "print(train_dataset)\n",
    "\n",
    "\n",
    "data_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "print(data_loader)\n",
    "\n",
    "#show labels of the dataset\n",
    "\n",
    "print(train_dataset.classes)\n",
    "print(len(train_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Networks\n",
    "def conv_block(c_in, c_out, k_size=4, stride=2, pad=1, use_bn=True, transpose=False):\n",
    "    module = []\n",
    "    if transpose:\n",
    "        module.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, bias=not use_bn))\n",
    "    else:\n",
    "        module.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=not use_bn))\n",
    "    if use_bn:\n",
    "        module.append(nn.BatchNorm2d(c_out))\n",
    "    return nn.Sequential(*module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=10, num_classes=10, label_embed_size=5, channels=3, conv_dim=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, label_embed_size)\n",
    "        self.tconv1 = conv_block(z_dim + label_embed_size, conv_dim * 8, pad=0, transpose=True)\n",
    "        self.tconv2 = conv_block(conv_dim * 8, conv_dim * 4, transpose=True)\n",
    "        self.tconv3 = conv_block(conv_dim * 4, conv_dim * 2, transpose=True)\n",
    "        self.tconv4 = conv_block(conv_dim * 2, conv_dim, transpose=True)\n",
    "        self.tconv5 = conv_block(conv_dim, channels, transpose=True, use_bn=False)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        x = x.reshape([x.shape[0], -1, 1, 1])\n",
    "        label_embed = self.label_embedding(label)\n",
    "        label_embed = label_embed.reshape([label_embed.shape[0], -1, 1, 1])\n",
    "        x = torch.cat((x, label_embed), dim=1)\n",
    "        x = F.relu(self.tconv1(x))\n",
    "        x = F.relu(self.tconv2(x))\n",
    "        x = F.relu(self.tconv3(x))\n",
    "        x = F.tanh(self.tconv4(x))\n",
    "        x = torch.tanh(self.tconv5(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=10, channels=3, conv_dim=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.image_size = 64\n",
    "        self.label_embedding = nn.Embedding(num_classes, self.image_size*self.image_size)\n",
    "        self.conv1 = conv_block(channels + 1, conv_dim, use_bn=False)\n",
    "        self.conv2 = conv_block(conv_dim, conv_dim * 2)\n",
    "        self.conv3 = conv_block(conv_dim * 2, conv_dim * 4)\n",
    "        self.conv4 = conv_block(conv_dim * 4, conv_dim * 8)\n",
    "        self.conv5 = conv_block(conv_dim * 8, 1, k_size=4, stride=1, pad=0, use_bn=False)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        alpha = 0.2\n",
    "        label_embed = self.label_embedding(label)\n",
    "        label_embed = label_embed.reshape([label_embed.shape[0], 1, self.image_size, self.image_size])\n",
    "        x = torch.cat((x, label_embed), dim=1)\n",
    "        x = F.leaky_relu(self.conv1(x), alpha)\n",
    "        x = F.leaky_relu(self.conv2(x), alpha)\n",
    "        x = F.leaky_relu(self.conv3(x), alpha)\n",
    "        x = F.leaky_relu(self.conv4(x), alpha)\n",
    "        x = torch.sigmoid(self.conv5(x))\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim=Z_DIM, num_classes=NUM_CLASSES, label_embed_size=LABEL_EMBED_SIZE, channels=CHANNELS)\n",
    "dis = Discriminator(num_classes=NUM_CLASSES, channels=CHANNELS)\n",
    "\n",
    "\n",
    "# Teste unitário para verificar a saída do gerador\n",
    "test_noise = torch.randn(1, Z_DIM)\n",
    "test_label = torch.LongTensor([1])  # Exemplo de label\n",
    "gen_output = gen(test_noise, test_label)\n",
    "print(\"Output shape from generator:\", gen_output.shape)\n",
    "\n",
    "# Teste unitário para o discriminador\n",
    "dis_output = dis(gen_output, test_label)\n",
    "print(\"Output from discriminator:\", dis_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim=Z_DIM, num_classes=NUM_CLASSES, label_embed_size=LABEL_EMBED_SIZE, channels=CHANNELS)\n",
    "dis = Discriminator(num_classes=NUM_CLASSES, channels=CHANNELS)\n",
    "\n",
    "# Load previous model   \n",
    "if LOAD_MODEL:\n",
    "    gen.load_state_dict(torch.load(os.path.join(model_path, 'gen.pkl')))\n",
    "    dis.load_state_dict(torch.load(os.path.join(model_path, 'dis.pkl')))\n",
    "    \n",
    "# Model Summary\n",
    "print(\"------------------Generator------------------\")\n",
    "print(gen)\n",
    "print(\"------------------Discriminator------------------\")\n",
    "print(dis)\n",
    "\n",
    "# Define Optimizers\n",
    "g_opt = optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999), weight_decay=2e-5)\n",
    "d_opt = optim.Adam(dis.parameters(), lr=0.0002, betas=(0.5, 0.999), weight_decay=2e-5)\n",
    "\n",
    "# Loss functions\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Fix images for viz\n",
    "fixed_z = torch.randn(IMGS_TO_DISPLAY_PER_CLASS*NUM_CLASSES, Z_DIM)\n",
    "fixed_label = torch.arange(0, NUM_CLASSES)\n",
    "fixed_label = torch.repeat_interleave(fixed_label, IMGS_TO_DISPLAY_PER_CLASS)\n",
    "\n",
    "\n",
    "# GPU Compatibility\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    gen, dis = gen.cuda(), dis.cuda()\n",
    "    real_label, fake_label = real_label.cuda(), fake_label.cuda()\n",
    "    fixed_z, fixed_label = fixed_z.cuda(), fixed_label.cuda()\n",
    "\n",
    "total_iters = 0\n",
    "max_iter = len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for epoch in range(EPOCHS):\n",
    "    gen.train()\n",
    "    dis.train()\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "\n",
    "        total_iters += 1\n",
    "\n",
    "        x_real, x_label = data\n",
    "        batch_size = x_real.size(0)  # Tamanho do batch atual\n",
    "\n",
    "        # Ajustando o tamanho de z_fake para corresponder ao tamanho do batch atual\n",
    "        z_fake = torch.randn(batch_size, Z_DIM)\n",
    "        \n",
    "        real_label = torch.ones(batch_size)\n",
    "        fake_label = torch.zeros(batch_size)\n",
    "\n",
    "        if is_cuda:\n",
    "            x_real = x_real.cuda()\n",
    "            x_label = x_label.cuda()\n",
    "            z_fake = z_fake.cuda()\n",
    "                \n",
    "        # Generate fake data\n",
    "        x_fake = gen(z_fake, x_label)\n",
    "\n",
    "        # Train Discriminator\n",
    "        fake_out = dis(x_fake.detach(), x_label)\n",
    "        real_out = dis(x_real.detach(), x_label)\n",
    "        \n",
    "        d_loss = (loss_fn(fake_out, fake_label) + loss_fn(real_out, real_label)) / 2\n",
    "\n",
    "        d_opt.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_opt.step()\n",
    "\n",
    "        # Train Generator\n",
    "        fake_out = dis(x_fake, x_label)\n",
    "        g_loss = loss_fn(fake_out, real_label)\n",
    "\n",
    "        g_opt.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_opt.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(\"Epoch: \" + str(epoch + 1) + \"/\" + str(EPOCHS)\n",
    "                  + \"\\titer: \" + str(i) + \"/\" + str(max_iter)\n",
    "                  + \"\\ttotal_iters: \" + str(total_iters)\n",
    "                  + \"\\td_loss:\" + str(round(d_loss.item(), 4))\n",
    "                  + \"\\tg_loss:\" + str(round(g_loss.item(), 4))\n",
    "                  )\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save(gen.state_dict(), os.path.join(model_path, 'gen.pkl'))\n",
    "        torch.save(dis.state_dict(), os.path.join(model_path, 'dis.pkl'))\n",
    "\n",
    "        # generate_imgs(fixed_z, fixed_label, epoch=epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_imgs_for_all_labels(z, gen, num_labels, epoch=0):\n",
    "    gen.eval()\n",
    "    plt.figure(figsize=(15, 15))  # Ajusta o tamanho da figura para acomodar várias subfiguras\n",
    "\n",
    "    for label in range(num_labels):\n",
    "        fixed_label = torch.full((z.size(0),), label, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            fake_imgs = gen(z, fixed_label)\n",
    "        fake_imgs = (fake_imgs + 1) / 2  # Normaliza as imagens para o intervalo [0, 1]\n",
    "        fake_imgs_grid = vutils.make_grid(fake_imgs, normalize=False, nrow=IMGS_TO_DISPLAY_PER_CLASS)\n",
    "\n",
    "        # Converter tensor de imagens para um formato que o matplotlib possa exibir\n",
    "        np_img = fake_imgs_grid.permute(1, 2, 0).numpy()  # Reordenar as dimensões para HxWxC\n",
    "\n",
    "        # Cria uma subfigura para cada label\n",
    "        ax = plt.subplot(num_labels//2 + num_labels%2, 2, label + 1)  # Ajusta o layout da subplot\n",
    "        ax.imshow(np_img)\n",
    "        ax.set_title(f'Label {label}')\n",
    "        ax.axis('off')  # Desativa os eixos\n",
    "\n",
    "    plt.show()  # Mostra a imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img_for_label_after_training(z, label, gen_model, epoch=0):\n",
    "    gen_model.eval()\n",
    "\n",
    "    # Cria um tensor de labels com a label desejada\n",
    "    fixed_label = torch.full((1,), label, dtype=torch.long)  # Mudança para gerar apenas uma imagem\n",
    "    \n",
    "    # Gera a imagem com a label específica\n",
    "    with torch.no_grad():\n",
    "        fake_img = gen_model(z[0:1], fixed_label)  # Seleciona apenas um z para geração\n",
    "    fake_img = (fake_img + 1) / 2  # Normalização para adequar ao matplotlib\n",
    "    \n",
    "    # Converter tensor de imagens para um formato que o matplotlib possa exibir\n",
    "    np_img = fake_img.squeeze(0).permute(1, 2, 0).numpy()  # Remove batch dim e reordena para HxWxC\n",
    "    plt.figure(figsize=(1, 1))  # Define o tamanho da figura\n",
    "    plt.imshow(np_img)\n",
    "    plt.axis('off')  # Desativa os eixos\n",
    "    plt.show()  # Mostra a imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(10, 100)\n",
    "\n",
    "generate_imgs_for_all_labels(z, gen, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_generate = 9\n",
    "generate_img_for_label_after_training(fixed_z, label_to_generate, gen, epoch=epoch + 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
